{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a03b8d",
   "metadata": {},
   "source": [
    "# 将数据按照ID进行分别存储\n",
    "\n",
    "在同时使用multiprocessing 和 polars的时候，**千万注意不能将lazyframe数据放入多个进程中，会导致死锁**。LazyFrame 是惰性执行的，而 multiprocessing 会复制主进程的内存结构（使用 pickle 序列化对象）。但是：LazyFrame 是不可序列化的对象（不能 pickling），传入子进程后可能会触发隐式的强制 .collect() 或失败重算，导致每个进程都重新加载所有数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa369671",
   "metadata": {},
   "source": [
    "## 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = [\"ID\", \"time\", \"ID2\", 'longitude', 'latitude', 'geohash', 'epoch0', 'epoch1']\n",
    "df = pd.read_csv('./Data/Input/data.csv', sep=',', nrows=4096, header=None, names=column_names)\n",
    "\n",
    "chunk_size = 1000\n",
    "for i, chunk in enumerate(df.groupby(df.index // chunk_size)):\n",
    "    filename = './Data/Input/test/data_chunk_{}.csv'.format(i)\n",
    "    chunk[1].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb28d8",
   "metadata": {},
   "source": [
    "## 按ID分别存储所有数据\n",
    "\n",
    "原始的数据的主键是时间，也就是说按照时间来记录所有用户的轨迹。\n",
    "\n",
    "现在需要先按照ID来进行分组，然后再按照时间进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b439491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 6 files, loading...\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def group_and_save_by_id(data_dir, \n",
    "                         log_file,\n",
    "                         id_col=\"ID\", \n",
    "                         output_dir=\"output_by_id\", \n",
    "                         column_names=None):\n",
    "    data_dir = Path(data_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 获取所有文件路径\n",
    "    file_list = [str(f) for f in data_dir.iterdir() if f.is_file()]\n",
    "    print(f\"found {len(file_list)} files, loading...\")\n",
    "\n",
    "    # 读取并合并所有 CSV 文件，强制指定列名（如果列名为空）\n",
    "    lazy_frames = [\n",
    "        pl.read_csv(f, \n",
    "                    has_header=False, \n",
    "                    new_columns=column_names, \n",
    "                    schema_overrides={id_col: pl.Utf8}).lazy()\n",
    "        for f in file_list\n",
    "    ]\n",
    "    full_lf = pl.concat(lazy_frames)\n",
    "    # df = full_lf.collect()\n",
    "    \n",
    "    # 获取唯一 ID\n",
    "    unique_ids = (\n",
    "        full_lf.select(pl.col(id_col).cast(pl.Utf8).unique())\n",
    "        .collect()\n",
    "        .get_column(id_col)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    # 遍历分组并保存\n",
    "    for uid in unique_ids:\n",
    "        df = full_lf.filter(pl.col(id_col) == uid).collect()\n",
    "        out_path = f\"{output_dir}/{uid}.csv\"\n",
    "        df.write_csv(out_path)\n",
    "        print(f\"{uid} save {df.shape[0]} rows.\")\n",
    "        \n",
    "        # 写入日志。\n",
    "        # 判断是否需要写入表头。\n",
    "        write_header_flag = not os.path.exists(log_file) or os.stat(log_file).st_size == 0\n",
    "        with open(log_file, mode='a', newline='', encoding='utf-8') as log_f:\n",
    "            if write_header_flag:\n",
    "                writer.writerow([\"ID\", \"NumberofRow\"])\n",
    "            writer = csv.writer(log_f)\n",
    "            writer.writerow([uid, df.shape[0]])\n",
    "            \n",
    "\n",
    "    print(\"completed.\")\n",
    "\n",
    "\n",
    "column_names = [\"ID\", \"time\", \"ID2\", 'longitude', 'latitude', 'geohash', 'epoch0', 'epoch1']\n",
    "group_and_save_by_id(\n",
    "    data_dir=\"./Data/Input/\",\n",
    "    log_file='./Data/Output/log.csv',\n",
    "    id_col=\"ID\",\n",
    "    output_dir=\"./Data/Output/\",\n",
    "    column_names=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261e22d",
   "metadata": {},
   "source": [
    "## 将用户ID映射为整数\n",
    "\n",
    "由于用户ID为字符串，在放入tensor的时候必须为数值，所以需要将用户ID转换为整数，并且保留对应关系。\n",
    "\n",
    "只额吉将保存的log.csv中的保存顺序作为对应关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607e65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log = pd.read_csv('./Data/Geolife Trajectories 1.4/log.csv', header=0)\n",
    "log['IDMap'] = log.index\n",
    "log.to_csv('./Data/Geolife Trajectories 1.4/log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a1ab6",
   "metadata": {},
   "source": [
    "### 将所有用户的停留点数据中的用户ID进行映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2546e8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
