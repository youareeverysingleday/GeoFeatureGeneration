{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a03b8d",
   "metadata": {},
   "source": [
    "# 将数据按照ID进行分别存储\n",
    "\n",
    "在同时使用multiprocessing 和 polars的时候，**千万注意不能将lazyframe数据放入多个进程中，会导致死锁**。LazyFrame 是惰性执行的，而 multiprocessing 会复制主进程的内存结构（使用 pickle 序列化对象）。但是：LazyFrame 是不可序列化的对象（不能 pickling），传入子进程后可能会触发隐式的强制 .collect() 或失败重算，导致每个进程都重新加载所有数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa369671",
   "metadata": {},
   "source": [
    "## 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./Data/Input/data.csv', sep=',', nrows=4096, header=None)\n",
    "\n",
    "chunk_size = 1000\n",
    "for i, chunk in enumerate(df.groupby(df.index // chunk_size)):\n",
    "    filename = './Data/Input/test/data_chunk_{}.csv'.format(i)\n",
    "    chunk[1].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb28d8",
   "metadata": {},
   "source": [
    "## 按ID分别存储所有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b439491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def group_and_save_by_id(data_dir, \n",
    "                         log_file,\n",
    "                         id_col=\"ID\", \n",
    "                         output_dir=\"output_by_id\", \n",
    "                         column_names=None):\n",
    "    data_dir = Path(data_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 获取所有文件路径\n",
    "    file_list = [str(f) for f in data_dir.iterdir() if f.is_file()]\n",
    "    print(f\"找到 {len(file_list)} 个文件，开始加载...\")\n",
    "\n",
    "    # 读取并合并所有 CSV 文件，强制指定列名（如果列名为空）\n",
    "    lazy_frames = [\n",
    "        pl.read_csv(f, \n",
    "                    has_header=False, \n",
    "                    new_columns=column_names, \n",
    "                    schema_overrides={id_col: pl.Utf8}).lazy()\n",
    "        for f in file_list\n",
    "    ]\n",
    "    full_lf = pl.concat(lazy_frames)\n",
    "    # df = full_lf.collect()\n",
    "    \n",
    "    # 获取唯一 ID\n",
    "    unique_ids = (\n",
    "        full_lf.select(pl.col(id_col).cast(pl.Utf8).unique())\n",
    "        .collect()\n",
    "        .get_column(id_col)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    # 遍历分组并保存\n",
    "    for uid in unique_ids:\n",
    "        df = full_lf.filter(pl.col(id_col) == uid).collect()\n",
    "        out_path = f\"{output_dir}/{uid}.csv\"\n",
    "        df.write_csv(out_path)\n",
    "        print(f\"{uid} save {df.shape[0]} rows.\")\n",
    "        \n",
    "        # 写入日志。\n",
    "        # 判断是否需要写入表头。\n",
    "        write_header_flag = not os.path.exists(log_file) or os.stat(log_file).st_size == 0\n",
    "        with open(log_file, mode='a', newline='', encoding='utf-8') as log_f:\n",
    "            writer = csv.writer(log_f)\n",
    "            writer.writerow([uid, ])\n",
    "            if write_header_flag:\n",
    "                writer.writerow([\"ID\", \"NumberofRow\"])\n",
    "\n",
    "    print(\"全部保存完成 ✅\")\n",
    "\n",
    "\n",
    "column_names = [\"ID\", \"time\", \"phoneNumber\", 'longitude', 'latitude', 'geohash', 'epoch0', 'epoch1']\n",
    "group_and_save_by_id(\n",
    "    data_dir=\"./Data/Input/\",\n",
    "    id_col=\"ID\",\n",
    "    output_dir=\"./Data/Output/\",\n",
    "    column_names=column_names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
